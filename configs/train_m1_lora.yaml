training:
  seed: 42
  epochs: 3
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  max_grad_norm: 1.0

hardware:
  device: mps
  precision: fp16
  gradient_checkpointing: true

batching:
  micro_batch_size: 1
  gradient_accumulation_steps: 32
  eval_batch_size: 1

lora:
  enabled: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: [q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj]

loss:
  router_supervision_weight: 0.2
  load_balance_weight: 0.02

data:
  train_path: data/processed/train.jsonl
  val_path: data/processed/val.jsonl
  max_length: 1024

outputs:
  output_dir: artifacts
  save_steps: 500
  eval_steps: 500
  logging_steps: 5
